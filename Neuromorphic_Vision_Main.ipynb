{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOJ53GVLLxGam7iRZYJnRxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akobabs/Neuromorphic-Vision/blob/main/Neuromorphic_Vision_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "%matplotlib inline\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import struct\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Install required Python packages\n",
        "try:\n",
        "    import dv\n",
        "except ImportError:\n",
        "    !pip install --force-reinstall numpy pandas matplotlib torch norse scikit-learn seaborn tqdm dv\n",
        "    import dv\n",
        "\n",
        "# Mount Google Drive (for Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    logger.info(\"Not running in Colab, skipping drive mount\")\n",
        "\n",
        "# Verify data path\n",
        "data_path = '/content/drive/MyDrive/Neuromorphic Vision'\n",
        "if not os.path.exists(data_path):\n",
        "    logger.error(f\"Data path {data_path} does not exist\")\n",
        "else:\n",
        "    logger.info(f\"Data path {data_path} verified\")"
      ],
      "metadata": {
        "id": "lcuCvqZJmo7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Configuration\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration class for the neuromorphic vision pipeline\"\"\"\n",
        "    # Paths\n",
        "    data_path: str = '/content/drive/MyDrive/Neuromorphic Vision'\n",
        "\n",
        "    # DVS Gesture parameters\n",
        "    dvs_max_x: int = 128\n",
        "    dvs_max_y: int = 128\n",
        "    dvs_num_classes: int = 11\n",
        "\n",
        "    # N-Caltech101 parameters\n",
        "    caltech_max_x: int = 304\n",
        "    caltech_max_y: int = 240\n",
        "    caltech_num_classes: int = 101\n",
        "\n",
        "    # Preprocessing parameters\n",
        "    max_jitter: int = 100\n",
        "    time_bin: float = 0.01\n",
        "    temporal_window: float = 50000  # microseconds\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size: int = 4\n",
        "    learning_rate: float = 1e-3\n",
        "    num_epochs: int = 50\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Noise detection\n",
        "    dbscan_eps: float = 5.0\n",
        "    dbscan_min_samples: int = 10\n",
        "\n",
        "config = Config()\n",
        "logger.info(f\"Configuration initialized with device: {config.device}\")"
      ],
      "metadata": {
        "id": "IzMjFSFfmp3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Imports and Class Definitions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import norse.torch as norse\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import warnings\n",
        "import struct\n",
        "warnings.filterwarnings('ignore')\n",
        "from typing import Tuple, Optional, List, Dict, Any\n",
        "\n",
        "# DVSGestureProcessor\n",
        "class DVSGestureProcessor:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def load_dvs_gesture(self, aedat_file: str, csv_file: str) -> Tuple[Optional[np.ndarray], Optional[pd.DataFrame]]:\n",
        "      \"\"\"Load DVS gesture data using manual AEDAT 3.1 parsing\"\"\"\n",
        "      try:\n",
        "          with open(aedat_file, 'rb') as f:\n",
        "              # Skip header\n",
        "              header = b''\n",
        "              while not header.endswith(b'#!END-HEADER\\r\\n'):\n",
        "                  header += f.read(1)\n",
        "                  if len(header) > 1000:  # Safety check\n",
        "                      raise ValueError(\"Invalid AEDAT file: header too long\")\n",
        "\n",
        "              # Read events\n",
        "              events = []\n",
        "              while True:\n",
        "                  data = f.read(8)  # 4 bytes data + 4 bytes timestamp\n",
        "                  if len(data) < 8:\n",
        "                      break\n",
        "                  data_val, timestamp = struct.unpack('<II', data)\n",
        "                  x = (data_val >> 17) & 0x1FFF\n",
        "                  y = (data_val >> 2) & 0x1FFF\n",
        "                  polarity = (data_val >> 1) & 0x01\n",
        "                  events.append([timestamp, x, y, polarity])\n",
        "\n",
        "          event_array = np.array(events, dtype=np.float64)\n",
        "\n",
        "          # Load labels\n",
        "          labels = pd.read_csv(csv_file, names=['class', 'startTime_usec', 'endTime_usec'])\n",
        "\n",
        "          logger.info(f\"Loaded {len(event_array)} events and {len(labels)} labels from {aedat_file}\")\n",
        "          return event_array, labels\n",
        "\n",
        "      except Exception as e:\n",
        "          logger.error(f\"Error loading DVS gesture data from {aedat_file}: {e}\")\n",
        "          return None, None\n",
        "\n",
        "    def detect_noise_advanced(self, events: np.ndarray) -> Tuple[np.ndarray, Dict[str, float]]:\n",
        "        noise_mask = np.zeros(len(events), dtype=bool)\n",
        "        stats = {}\n",
        "        coords = events[:, 1:3]\n",
        "        if len(coords) > 0:\n",
        "            clustering = DBSCAN(eps=self.config.dbscan_eps,\n",
        "                              min_samples=self.config.dbscan_min_samples).fit(coords)\n",
        "            spatial_noise = clustering.labels_ == -1\n",
        "            noise_mask |= spatial_noise\n",
        "            stats['spatial_noise_rate'] = np.mean(spatial_noise)\n",
        "        if len(events) > 1:\n",
        "            time_diffs = np.diff(events[:, 0])\n",
        "            temporal_threshold = np.percentile(time_diffs, 95)\n",
        "            isolated_events = np.concatenate([[False], time_diffs > temporal_threshold])\n",
        "            noise_mask |= isolated_events\n",
        "            stats['temporal_noise_rate'] = np.mean(isolated_events)\n",
        "        pixel_counts = {}\n",
        "        for x, y in events[:, 1:3]:\n",
        "            pixel_counts[(x, y)] = pixel_counts.get((x, y), 0) + 1\n",
        "        if pixel_counts:\n",
        "            count_threshold = np.percentile(list(pixel_counts.values()), 99)\n",
        "            hot_pixel_mask = np.array([pixel_counts[(x, y)] > count_threshold\n",
        "                                     for x, y in events[:, 1:3]])\n",
        "            noise_mask |= hot_pixel_mask\n",
        "            stats['hot_pixel_rate'] = np.mean(hot_pixel_mask)\n",
        "        stats['total_noise_rate'] = np.mean(noise_mask)\n",
        "        logger.info(f\"Noise detection stats: {stats}\")\n",
        "        return events[~noise_mask], stats\n",
        "\n",
        "    def preprocess_dvs_gesture(self, events: np.ndarray, labels: pd.DataFrame) -> np.ndarray:\n",
        "        if events is None or len(events) == 0:\n",
        "            return np.array([])\n",
        "        valid_mask = np.zeros(len(events), dtype=bool)\n",
        "        for _, row in labels.iterrows():\n",
        "            start_time, end_time = row['startTime_usec'], row['endTime_usec']\n",
        "            window_mask = (events[:, 0] >= start_time) & (events[:, 0] <= end_time)\n",
        "            valid_mask |= window_mask\n",
        "        events = events[valid_mask]\n",
        "        spatial_mask = ((events[:, 1] >= 0) & (events[:, 1] < self.config.dvs_max_x) &\n",
        "                       (events[:, 2] >= 0) & (events[:, 2] < self.config.dvs_max_y))\n",
        "        events = events[spatial_mask]\n",
        "        if len(events) == 0:\n",
        "            return events\n",
        "        t_min, t_max = events[:, 0].min(), events[:, 0].max()\n",
        "        if t_max > t_min:\n",
        "            events[:, 0] = (events[:, 0] - t_min) / (t_max - t_min)\n",
        "            if self.config.max_jitter > 0:\n",
        "                jitter = np.random.uniform(-self.config.max_jitter / (t_max - t_min),\n",
        "                                         self.config.max_jitter / (t_max - t_min),\n",
        "                                         len(events))\n",
        "                events[:, 0] = np.clip(events[:, 0] + jitter, 0, 1)\n",
        "        return events\n",
        "\n",
        "    def visualize_dvs_data(self, events: np.ndarray, labels: pd.DataFrame, save_path: Optional[str] = None):\n",
        "        if events is None or len(events) == 0:\n",
        "            logger.warning(\"No events to visualize\")\n",
        "            return\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        axes[0, 0].hist(events[:, 0], bins=100, color='blue', alpha=0.7, edgecolor='black')\n",
        "        axes[0, 0].set_xlabel('Normalized Timestamp')\n",
        "        axes[0, 0].set_ylabel('Event Count')\n",
        "        axes[0, 0].set_title('DVS Gesture Temporal Distribution')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        scatter = axes[0, 1].scatter(events[:, 1], events[:, 2], s=0.5,\n",
        "                                   c=events[:, 3], cmap='Blues', alpha=0.6)\n",
        "        axes[0, 1].set_xlabel('X (pixels)')\n",
        "        axes[0, 1].set_ylabel('Y (pixels)')\n",
        "        axes[0, 1].set_title('DVS Gesture Spatial Distribution')\n",
        "        plt.colorbar(scatter, ax=axes[0, 1], label='Polarity (0: OFF, 1: ON)')\n",
        "        class_counts = labels['class'].value_counts().sort_index()\n",
        "        axes[1, 0].bar(class_counts.index, class_counts.values,\n",
        "                      color='skyblue', edgecolor='black')\n",
        "        axes[1, 0].set_xlabel('Gesture Class')\n",
        "        axes[1, 0].set_ylabel('Count')\n",
        "        axes[1, 0].set_title('DVS Gesture Class Distribution')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        time_bins = np.linspace(0, 1, 50)\n",
        "        event_rates = []\n",
        "        for i in range(len(time_bins) - 1):\n",
        "            mask = (events[:, 0] >= time_bins[i]) & (events[:, 0] < time_bins[i + 1])\n",
        "            event_rates.append(np.sum(mask))\n",
        "        axes[1, 1].plot(time_bins[:-1], event_rates, color='blue', linewidth=2)\n",
        "        axes[1, 1].set_xlabel('Normalized Time')\n",
        "        axes[1, 1].set_ylabel('Event Rate')\n",
        "        axes[1, 1].set_title('Event Rate Over Time')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "# Updated NCaltech101Processor\n",
        "class NCaltech101Processor:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def load_ncaltech101(self, bin_file: str) -> np.ndarray:\n",
        "        \"\"\"Load N-Caltech101 data (40-bit events)\"\"\"\n",
        "        try:\n",
        "            with open(bin_file, 'rb') as f:\n",
        "                data = f.read()\n",
        "            events = []\n",
        "            for i in range(0, len(data), 5):\n",
        "                if i + 5 <= len(data):\n",
        "                    # Unpack 40 bits: 8-bit X, 8-bit Y, 1-bit polarity, 23-bit timestamp\n",
        "                    event = struct.unpack('<BBHB', data[i:i+5])\n",
        "                    x = event[0]\n",
        "                    y = event[1]\n",
        "                    polarity = (event[2] >> 7) & 0x01\n",
        "                    timestamp = ((event[2] & 0x7F) << 16) | event[3]\n",
        "                    events.append([timestamp, x, y, polarity])\n",
        "            events = np.array(events, dtype=np.float64)\n",
        "            logger.info(f\"Loaded {len(events)} events from {bin_file}\")\n",
        "            return events\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading N-Caltech101 data from {bin_file}: {e}\")\n",
        "            return np.array([])\n",
        "\n",
        "    def preprocess_ncaltech101(self, events: np.ndarray) -> np.ndarray:\n",
        "        if len(events) == 0:\n",
        "            return events\n",
        "        spatial_mask = ((events[:, 1] >= 0) & (events[:, 1] < self.config.caltech_max_x) &\n",
        "                       (events[:, 2] >= 0) & (events[:, 2] < self.config.caltech_max_y))\n",
        "        events = events[spatial_mask]\n",
        "        if len(events) == 0:\n",
        "            return events\n",
        "        t_min, t_max = events[:, 0].min(), events[:, 0].max()\n",
        "        if t_max > t_min:\n",
        "            events[:, 0] = (events[:, 0] - t_min) / (t_max - t_min)\n",
        "            if self.config.max_jitter > 0:\n",
        "                jitter = np.random.uniform(-self.config.max_jitter / (t_max - t_min),\n",
        "                                         self.config.max_jitter / (t_max - t_min),\n",
        "                                         len(events))\n",
        "                events[:, 0] = np.clip(events[:, 0] + jitter, 0, 1)\n",
        "        return events\n",
        "\n",
        "# Other classes (unchanged)\n",
        "class DataProcessor:\n",
        "    @staticmethod\n",
        "    def setup_environment():\n",
        "        try:\n",
        "            import loris\n",
        "        except ImportError:\n",
        "            os.system('pip install loris numpy pandas matplotlib torch norse scikit-learn seaborn tqdm')\n",
        "            import loris\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "        except ImportError:\n",
        "            logger.info(\"Not running in Colab, skipping drive mount\")\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_file_paths(file_paths: List[str]) -> List[str]:\n",
        "        valid_paths = []\n",
        "        for path in file_paths:\n",
        "            if os.path.exists(path):\n",
        "                valid_paths.append(path)\n",
        "            else:\n",
        "                logger.warning(f\"File not found: {path}\")\n",
        "        return valid_paths\n",
        "\n",
        "class EventFrameConverter:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def events_to_frames(self, events: np.ndarray, shape: Tuple[int, int, int],\n",
        "                        method: str = 'histogram') -> np.ndarray:\n",
        "        if len(events) == 0:\n",
        "            return np.zeros((1, *shape))\n",
        "        time_bins = np.arange(0, 1 + self.config.time_bin, self.config.time_bin)\n",
        "        frames = []\n",
        "        for i in range(len(time_bins) - 1):\n",
        "            t_start, t_end = time_bins[i], time_bins[i + 1]\n",
        "            mask = (events[:, 0] >= t_start) & (events[:, 0] < t_end)\n",
        "            frame_events = events[mask]\n",
        "            if method == 'histogram':\n",
        "                frame = self._histogram_representation(frame_events, shape)\n",
        "            elif method == 'time_surface':\n",
        "                frame = self._time_surface_representation(frame_events, shape, t_end)\n",
        "            else:\n",
        "                frame = self._histogram_representation(frame_events, shape)\n",
        "            frames.append(frame)\n",
        "        return np.array(frames)\n",
        "\n",
        "    def _histogram_representation(self, events: np.ndarray, shape: Tuple[int, int, int]) -> np.ndarray:\n",
        "        frame = np.zeros(shape)\n",
        "        if len(events) > 0:\n",
        "            for _, x, y, p in events:\n",
        "                if 0 <= int(x) < shape[1] and 0 <= int(y) < shape[0]:\n",
        "                    frame[int(y), int(x), int(p)] += 1\n",
        "        return frame\n",
        "\n",
        "    def _time_surface_representation(self, events: np.ndarray, shape: Tuple[int, int, int],\n",
        "                                   current_time: float) -> np.ndarray:\n",
        "        frame = np.zeros(shape)\n",
        "        if len(events) > 0:\n",
        "            for t, x, y, p in events:\n",
        "                if 0 <= int(x) < shape[1] and 0 <= int(y) < shape[0]:\n",
        "                    time_diff = current_time - t\n",
        "                    decay = np.exp(-time_diff / 0.1)\n",
        "                    frame[int(y), int(x), int(p)] = max(frame[int(y), int(x), int(p)], decay)\n",
        "        return frame\n",
        "\n",
        "class NeuromorphicDataset(Dataset):\n",
        "    def __init__(self, frames: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.frames = torch.FloatTensor(frames)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame = self.frames[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            frame = self.transform(frame)\n",
        "        return frame, label\n",
        "\n",
        "class EnhancedSpikingNet(nn.Module):\n",
        "    def __init__(self, input_shape: Tuple[int, int, int], num_classes: int, dropout_rate: float = 0.2):\n",
        "        super().__init__()\n",
        "        c, h, w = input_shape\n",
        "        self.features = nn.Sequential(\n",
        "            norse.LIConv2d(c, 32, kernel_size=3, padding=1),\n",
        "            norse.LIFCell(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(dropout_rate),\n",
        "            norse.LIConv2d(32, 64, kernel_size=3, padding=1),\n",
        "            norse.LIFCell(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(dropout_rate),\n",
        "            norse.LIConv2d(64, 128, kernel_size=3, padding=1),\n",
        "            norse.LIFCell(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.AdaptiveAvgPool2d((4, 4)),\n",
        "            nn.Dropout2d(dropout_rate),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            norse.LIFCell(128 * 4 * 4, 256),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            norse.LIFCell(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model: nn.Module, config: Config):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = torch.device(config.device)\n",
        "        self.model.to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def train_epoch(self, dataloader: DataLoader) -> Tuple[float, float]:\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += target.size(0)\n",
        "            accuracy = 100. * correct / total\n",
        "            progress_bar.set_postfix({'Loss': f'{loss.item():.4f}', 'Acc': f'{accuracy:.2f}%'})\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        accuracy = 100. * correct / total\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def validate(self, dataloader: DataLoader) -> Tuple[float, float]:\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in tqdm(dataloader, desc=\"Validation\"):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, target)\n",
        "                total_loss += loss.item()\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "                total += target.size(0)\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        accuracy = 100. * correct / total\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def train(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[str, List[float]]:\n",
        "        logger.info(f\"Starting training on {self.device}\")\n",
        "        best_val_acc = 0\n",
        "        patience_counter = 0\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            logger.info(f\"Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
        "            train_loss, train_acc = self.train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_acc)\n",
        "            val_loss, val_acc = self.validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "            self.scheduler.step(val_loss)\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "                torch.save(self.model.state_dict(), os.path.join(self.config.data_path, 'best_model.pth'))\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= 10:\n",
        "                    logger.info(\"Early stopping triggered\")\n",
        "                    break\n",
        "            logger.info(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "            logger.info(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "            logger.info(f\"Best Val Acc: {best_val_acc:.2f}%\")\n",
        "        return {\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "            'train_accuracies': self.train_accuracies,\n",
        "            'val_accuracies': self.val_accuracies\n",
        "        }\n",
        "\n",
        "    def plot_training_history(self, save_path: Optional[str] = None):\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "        ax1.plot(self.train_losses, label='Train Loss', color='blue')\n",
        "        ax1.plot(self.val_losses, label='Validation Loss', color='red')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        ax2.plot(self.train_accuracies, label='Train Accuracy', color='blue')\n",
        "        ax2.plot(self.val_accuracies, label='Validation Accuracy', color='red')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy (%)')\n",
        "        ax2.set_title('Training and Validation Accuracy')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "class AdvancedAnalytics:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def evaluate_model(self, model: nn.Module, test_loader: DataLoader,\n",
        "                      class_names: List[str] = None) -> Dict[str, Any]:\n",
        "        model.eval()\n",
        "        device = next(model.parameters()).device\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        all_probs = []\n",
        "        with torch.no_grad():\n",
        "            for data, target in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "                data, target = data.to(device), target.to(self.config.device)\n",
        "                output = model(data)\n",
        "                probs = torch.softmax(output, dim=1)\n",
        "                pred = output.argmax(dim=1)\n",
        "                all_preds.extend(pred.cpu().numpy())\n",
        "                all_targets.extend(target.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "        accuracy = np.mean(np.array(all_preds) == np.array(all_targets))\n",
        "        if class_names is None:\n",
        "            class_names = [f\"Class_{i}\" for i in range(len(np.unique(all_targets)))]\n",
        "        report = classification_report(all_targets, all_preds, target_names=class_names, output_dict=True)\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'classification_report': report,\n",
        "            'confusion_matrix': cm,\n",
        "            'predictions': all_preds,\n",
        "            'targets': all_targets,\n",
        "            'probabilities': all_probs\n",
        "        }\n",
        "\n",
        "    def plot_confusion_matrix(self, cm: np.ndarray, class_names: List[str], save_path: Optional[str] = None):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def analyze_event_statistics(self, events: np.ndarray) -> Dict[str, Any]:\n",
        "        if len(events) == 0:\n",
        "            return {}\n",
        "        stats = {\n",
        "            'total_events': len(events),\n",
        "            'temporal_span': events[:, 0].max() - events[:, 0].min(),\n",
        "            'spatial_span_x': events[:, 1].max() - events[:, 1].min(),\n",
        "            'spatial_span_y': events[:, 2].max() - events[:, 2].min(),\n",
        "            'polarity_ratio': np.mean(events[:, 3]),\n",
        "            'event_rate': len(events) / (events[:, 0].max() - events[:, 0].min() + 1e-6),\n",
        "            'spatial_density': len(events) / ((events[:, 1].max() - events[:, 1].min() + 1) *\n",
        "                                            (events[:, 2].max() - events[:, 2].min() + 1))\n",
        "        }\n",
        "        if len(events) > 1:\n",
        "            inter_event_times = np.diff(events[:, 0])\n",
        "            stats.update({\n",
        "                'mean_inter_event_time': np.mean(inter_event_times),\n",
        "                'std_inter_event_time': np.std(inter_event_times),\n",
        "                'median_inter_event_time': np.median(inter_event_times)\n",
        "            })\n",
        "        return stats\n",
        "\n",
        "class DataAugmentation:\n",
        "    @staticmethod\n",
        "    def spatial_jitter(events: np.ndarray, max_shift: int = 5) -> np.ndarray:\n",
        "        augmented_events = events.copy()\n",
        "        shift_x = np.random.randint(-max_shift, max_shift + 1)\n",
        "        shift_y = np.random.randint(-max_shift, max_shift + 1)\n",
        "        augmented_events[:, 1] += shift_x\n",
        "        augmented_events[:, 2] += shift_y\n",
        "        return augmented_events\n",
        "\n",
        "    @staticmethod\n",
        "    def temporal_stretch(events: np.ndarray, stretch_factor: float = 1.2) -> np.ndarray:\n",
        "        augmented_events = events.copy()\n",
        "        augmented_events[:, 0] *= stretch_factor\n",
        "        return augmented_events\n",
        "\n",
        "    @staticmethod\n",
        "    def polarity_flip(events: np.ndarray, flip_prob: float = 0.1) -> np.ndarray:\n",
        "        augmented_events = events.copy()\n",
        "        flip_mask = np.random.random(len(events)) < flip_prob\n",
        "        augmented_events[flip_mask, 3] = 1 - augmented_events[flip_mask, 3]\n",
        "        return augmented_events\n",
        "\n",
        "    @staticmethod\n",
        "    def event_dropout(events: np.ndarray, dropout_rate: float = 0.1) -> np.ndarray:\n",
        "        keep_mask = np.random.random(len(events)) > dropout_rate\n",
        "        return events[keep_mask]\n",
        "\n",
        "class BatchProcessor:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def process_dvs_dataset_batch(self, data_dir: str, output_dir: str, num_workers: int = 4) -> Dict[str, Any]:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        aedat_files = []\n",
        "        for root, dirs, files in os.walk(data_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.aedat'):\n",
        "                    aedat_path = os.path.join(root, file)\n",
        "                    csv_path = aedat_path.replace('.aedat', '_labels.csv')\n",
        "                    if os.path.exists(csv_path):\n",
        "                        aedat_files.append((aedat_path, csv_path))\n",
        "        logger.info(f\"Found {len(aedat_files)} AEDAT files with labels\")\n",
        "        processor = DVSGestureProcessor(self.config)\n",
        "        frame_converter = EventFrameConverter(self.config)\n",
        "        all_frames = []\n",
        "        all_labels = []\n",
        "        processing_stats = []\n",
        "\n",
        "        def process_single_file(file_pair):\n",
        "            aedat_file, csv_file = file_pair\n",
        "            try:\n",
        "                events, labels = processor.load_dvs_gesture(aedat_file, csv_file)\n",
        "                if events is None:\n",
        "                    return None\n",
        "                clean_events, noise_stats = processor.detect_noise_advanced(events)\n",
        "                processed_events = processor.preprocess_dvs_gesture(clean_events, labels)\n",
        "                frames = frame_converter.events_to_frames(\n",
        "                    processed_events,\n",
        "                    (self.config.dvs_max_y, self.config.dvs_max_x, 2)\n",
        "                )\n",
        "                frame_labels = []\n",
        "                t_min, t_max = events[:, 0].min(), events[:, 0].max()\n",
        "                time_bins = np.arange(0, 1 + self.config.time_bin, self.config.time_bin)\n",
        "                for i in range(len(time_bins) - 1):\n",
        "                    t_start = time_bins[i]\n",
        "                    t_end = time_bins[i + 1]\n",
        "                    event_mask = (processed_events[:, 0] >= t_start) & (processed_events[:, 0] < t_end)\n",
        "                    if np.any(event_mask):\n",
        "                        for _, row in labels.iterrows():\n",
        "                            start_time = (row['startTime_usec'] - t_min) / (t_max - t_min)\n",
        "                            end_time = (row['endTime_usec'] - t_min) / (t_max - t_min)\n",
        "                            if start_time <= t_end and end_time >= t_start:\n",
        "                                frame_labels.append(row['class'])\n",
        "                                break\n",
        "                        else:\n",
        "                            frame_labels.append(-1)\n",
        "                    else:\n",
        "                        frame_labels.append(-1)\n",
        "                frame_labels = frame_labels[:len(frames)]\n",
        "                valid_mask = np.array(frame_labels) != -1\n",
        "                frames = frames[valid_mask]\n",
        "                frame_labels = np.array(frame_labels)[valid_mask]\n",
        "                return {\n",
        "                    'frames': frames,\n",
        "                    'labels': frame_labels,\n",
        "                    'stats': noise_stats,\n",
        "                    'file': aedat_file\n",
        "                }\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing {aedat_file}: {e}\")\n",
        "                return None\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "            results = list(tqdm(executor.map(process_single_file, aedat_files), total=len(aedat_files), desc=\"Processing files\"))\n",
        "        valid_results = [r for r in results if r is not None]\n",
        "        for result in valid_results:\n",
        "            all_frames.append(result['frames'])\n",
        "            all_labels.extend(result['labels'])\n",
        "            processing_stats.append(result['stats'])\n",
        "        if all_frames:\n",
        "            all_frames = np.concatenate(all_frames, axis=0)\n",
        "            all_labels = np.array(all_labels)\n",
        "            np.save(os.path.join(output_dir, 'frames.npy'), all_frames)\n",
        "            np.save(os.path.join(output_dir, 'labels.npy'), all_labels)\n",
        "            with open(os.path.join(output_dir, 'processing_stats.pkl'), 'wb') as f:\n",
        "                pickle.dump(processing_stats, f)\n",
        "            logger.info(f\"Processed {len(all_frames)} frames from {len(valid_results)} files\")\n",
        "            return {\n",
        "                'frames': all_frames,\n",
        "                'labels': all_labels,\n",
        "                'stats': processing_stats,\n",
        "                'num_files': len(valid_results)\n",
        "            }\n",
        "        return None\n",
        "\n",
        "class ModelOptimizer:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def architecture_search(self, train_loader: DataLoader, val_loader: DataLoader,\n",
        "                          input_shape: Tuple[int, int, int], num_classes: int) -> Dict[str, Any]:\n",
        "        architectures = [\n",
        "            {'channels': [16, 32, 64], 'dropout_rate': 0.1},\n",
        "            {'channels': [32, 64, 128], 'dropout_rate': 0.2},\n",
        "            {'channels': [64, 128, 256], 'dropout_rate': 0.3},\n",
        "        ]\n",
        "        results = []\n",
        "        for i, arch_config in enumerate(architectures):\n",
        "            logger.info(f\"Testing architecture {i+1}/{len(architectures)}: {arch_config}\")\n",
        "            model = self._create_custom_model(input_shape, num_classes, arch_config)\n",
        "            trainer = Trainer(model, self.config)\n",
        "            temp_config = Config()\n",
        "            temp_config.num_epochs = 10\n",
        "            trainer.config = temp_config\n",
        "            history = trainer.train(train_loader, val_loader)\n",
        "            best_val_acc = max(history['val_accuracies'])\n",
        "            results.append({\n",
        "                'architecture': arch_config,\n",
        "                'best_val_accuracy': best_val_acc,\n",
        "                'history': history\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    def _create_custom_model(self, input_shape: Tuple[int], num_classes: int, config: Dict) -> nn.Module:\n",
        "        class CustomSpikingNet(nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                c, h, w = input_shape\n",
        "                channels = config['channels']\n",
        "                dropout_rate = config['dropout_rate']\n",
        "                layers = []\n",
        "                in_channels = c\n",
        "                for out_channels in channels:\n",
        "                    layers.extend([\n",
        "                        norse.LIConv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                        norse.LIFCell(),\n",
        "                        nn.BatchNorm2d(out_channels),\n",
        "                        nn.MaxPool2d(2),\n",
        "                        nn.Dropout2d(dropout_rate),\n",
        "                    ])\n",
        "                    in_channels = out_channels\n",
        "                self.features = nn.Sequential(*layers)\n",
        "                with torch.no_grad():\n",
        "                    dummy_input = torch.zeros(1, c, h, w)\n",
        "                    dummy_output = self.features(dummy_input)\n",
        "                    flattened_size = dummy_output.numel()\n",
        "                self.classifier = nn.Sequential(\n",
        "                    nn.Flatten(),\n",
        "                    norse.LIFCell(flattened_size, 256),\n",
        "                    nn.Dropout(dropout_rate),\n",
        "                    norse.LIFCell(256, num_classes)\n",
        "                )\n",
        "            def forward(self, x):\n",
        "                x = self.features(x)\n",
        "                x = self.classifier(x)\n",
        "                return x\n",
        "        return CustomSpikingNet()\n",
        "\n",
        "# Demo Pipeline Function\n",
        "def create_demo_pipeline():\n",
        "    logger.info(\"Creating demo pipeline with synthetic data\")\n",
        "    def generate_synthetic_events(num_events: int = 1000) -> np.ndarray:\n",
        "        events = np.zeros((num_events, 4))\n",
        "        events[:, 0] = np.sort(np.random.uniform(0, 100000, num_events))\n",
        "        events[:, 0] = (events[:, 0] - events[:, 0].min()) / (events[:, 0].max() - events[:, 0].min() + 1e-6)\n",
        "        cluster_centers = [(32, 32), (96, 32), (32, 96), (96, 96)]\n",
        "        for i in range(num_events):\n",
        "            center = cluster_centers[i % len(cluster_centers)]\n",
        "            events[i, 1] = np.clip(np.random.normal(center[0], 10), 0, 127)\n",
        "            events[i, 2] = np.clip(np.random.normal(center[1], 10), 0, 127)\n",
        "        events[:, 3] = np.random.choice([0, 1], num_events, p=[0.5, 0.5])\n",
        "        return events\n",
        "\n",
        "    all_frames = []\n",
        "    all_labels = []\n",
        "    frame_converter = EventFrameConverter(config)\n",
        "    for class_id in range(5):\n",
        "        for _ in range(20):\n",
        "            events = generate_synthetic_events(1000 + np.random.randint(-100, 100))\n",
        "            frames = frame_converter.events_to_frames(events, (config.dvs_max_y, config.dvs_max_x, 2))\n",
        "            all_frames.append(frames)\n",
        "            all_labels.extend([class_id] * len(frames))\n",
        "    all_frames = np.concatenate(all_frames, axis=0)\n",
        "    all_labels = np.array(all_labels)\n",
        "    logger.info(f\"Generated {len(all_frames)} synthetic frames with {len(np.unique(all_labels))} classes\")\n",
        "    X_train, X_val, y_train, y_val = train_test_split(all_frames, all_labels, test_size=0.2, random_state=42)\n",
        "    train_dataset = NeuromorphicDataset(X_train, y_train)\n",
        "    val_dataset = NeuromorphicDataset(X_val, y_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "    input_shape = (2, config.dvs_max_y, config.dvs_max_x)\n",
        "    model = EnhancedSpikingNet(input_shape, 5)\n",
        "    trainer = Trainer(model, config)\n",
        "    history = trainer.train(train_loader, val_loader)\n",
        "    trainer.plot_training_history(save_path='demo_training_history.png')\n",
        "    analytics = AdvancedAnalytics(config)\n",
        "    evaluation = analytics.evaluate_model(model, val_loader, class_names=[f\"Synthetic_{i}\" for i in range(5)])\n",
        "    analytics.plot_confusion_matrix(evaluation['confusion_matrix'], [f'Synthetic_{i}' for i in range(5)],\n",
        "                                  save_path='demo_confusion_matrix.png')\n",
        "    logger.info(f\"Demo Accuracy: {evaluation['accuracy']:.4f}\")\n",
        "    return {\n",
        "        'model': model,\n",
        "        'trainer': trainer,\n",
        "        'evaluation': evaluation,\n",
        "        'history': history\n",
        "    }\n",
        "\n",
        "logger.info(\"All classes and functions defined\")"
      ],
      "metadata": {
        "id": "wT2N_M26mtWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gc\n",
        "import logging\n",
        "import psutil\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "process = psutil.Process()\n",
        "\n",
        "def log_memory(prefix=\"\"):\n",
        "    mem = process.memory_info().rss / 1024 / 1024\n",
        "    logger.info(f\"[MEM] {prefix} RSS memory: {mem:.2f} MB\")\n",
        "\n",
        "# Cell 4: Process DVS Gesture Data or Load Preprocessed\n",
        "dvs_processor = DVSGestureProcessor(config)\n",
        "frame_converter = EventFrameConverter(config)\n",
        "analytics = AdvancedAnalytics(config)\n",
        "augmenter = DataAugmentation()\n",
        "\n",
        "frames_path = os.path.join(config.data_path, 'processed_data/frames.npy')\n",
        "labels_path = os.path.join(config.data_path, 'processed_data/labels.npy')\n",
        "\n",
        "try:\n",
        "    if os.path.exists(frames_path) and os.path.exists(labels_path):\n",
        "        histogram_frames = np.load(frames_path, mmap_mode='r')  # Use mmap to conserve memory\n",
        "        frame_labels = np.load(labels_path, mmap_mode='r')\n",
        "        logger.info(f\"Loaded preprocessed data: {len(histogram_frames)} frames\")\n",
        "        log_memory(\"After loading .npy\")\n",
        "\n",
        "    else:\n",
        "        aedat_file = os.path.join(config.data_path, 'DVS/DvsGesture/user10_fluorescent_led.aedat')\n",
        "        csv_file = os.path.join(config.data_path, 'DVS/DvsGesture/user10_fluorescent_led_labels.csv')\n",
        "\n",
        "        if os.path.exists(aedat_file) and os.path.exists(csv_file):\n",
        "            logger.info(\"Loading DVS Gesture data...\")\n",
        "            events, labels = dvs_processor.load_dvs_gesture(aedat_file, csv_file)\n",
        "            log_memory(\"After loading events\")\n",
        "\n",
        "            if events is None or len(labels) == 0:\n",
        "                raise ValueError(\"No events or labels loaded\")\n",
        "\n",
        "            event_stats = analytics.analyze_event_statistics(events)\n",
        "            logger.info(f\"Event statistics: {event_stats}\")\n",
        "\n",
        "            clean_events, noise_stats = dvs_processor.detect_noise_advanced(events)\n",
        "            logger.info(f\"Noise detection stats: {noise_stats}\")\n",
        "            del events  # Free memory early\n",
        "            gc.collect()\n",
        "\n",
        "            # Data augmentation\n",
        "            augmented_events = augmenter.spatial_jitter(clean_events, max_shift=3)\n",
        "            augmented_events = augmenter.temporal_stretch(augmented_events, stretch_factor=1.1)\n",
        "            del clean_events\n",
        "            gc.collect()\n",
        "\n",
        "            processed_events = dvs_processor.preprocess_dvs_gesture(augmented_events, labels)\n",
        "            dvs_processor.visualize_dvs_data(processed_events, labels, save_path='dvs_visualization.png')\n",
        "            log_memory(\"After preprocessing and visualization\")\n",
        "\n",
        "            # Convert to frames\n",
        "            histogram_frames = frame_converter.events_to_frames(\n",
        "                processed_events,\n",
        "                (config.dvs_max_y, config.dvs_max_x, 2),\n",
        "                method='histogram'\n",
        "            )\n",
        "            logger.info(f\"Generated {len(histogram_frames)} histogram frames\")\n",
        "            log_memory(\"After frame conversion\")\n",
        "\n",
        "            # Time binning and label mapping\n",
        "            frame_labels = []\n",
        "            time_bins = np.arange(0, 1 + config.time_bin, config.time_bin)\n",
        "            t_min, t_max = processed_events[:, 0].min(), processed_events[:, 0].max()\n",
        "\n",
        "            for i in range(len(time_bins) - 1):\n",
        "                t_start = time_bins[i]\n",
        "                t_end = time_bins[i + 1]\n",
        "                event_mask = (processed_events[:, 0] >= t_start) & (processed_events[:, 0] < t_end)\n",
        "\n",
        "                if np.any(event_mask):\n",
        "                    label_found = False\n",
        "                    for _, row in labels.iterrows():\n",
        "                        start_time = (row['startTime_usec'] - t_min) / (t_max - t_min)\n",
        "                        end_time = (row['endTime_usec'] - t_min) / (t_max - t_min)\n",
        "                        if start_time <= t_end and end_time >= t_start:\n",
        "                            frame_labels.append(row['class'])\n",
        "                            label_found = True\n",
        "                            break\n",
        "                    if not label_found:\n",
        "                        frame_labels.append(-1)\n",
        "                else:\n",
        "                    frame_labels.append(-1)\n",
        "\n",
        "            frame_labels = np.array(frame_labels[:len(histogram_frames)])\n",
        "            valid_mask = frame_labels != -1\n",
        "            histogram_frames = histogram_frames[valid_mask]\n",
        "            frame_labels = frame_labels[valid_mask]\n",
        "            log_memory(\"After label mapping\")\n",
        "\n",
        "            if len(histogram_frames) == 0:\n",
        "                raise ValueError(\"No valid frames after label mapping\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Missing .aedat or .csv file: {aedat_file} or {csv_file}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Pipeline failed: {e}. Falling back to demo pipeline.\")\n",
        "    histogram_frames = None\n",
        "    frame_labels = None\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "1p16sDRX3jPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Process DVS Gesture Data or Load Preprocessed\n",
        "dvs_processor = DVSGestureProcessor(config)\n",
        "frame_converter = EventFrameConverter(config)\n",
        "analytics = AdvancedAnalytics(config)\n",
        "augmenter = DataAugmentation()\n",
        "\n",
        "# Check for preprocessed data\n",
        "frames_path = os.path.join(config.data_path, 'processed_data/frames.npy')\n",
        "labels_path = os.path.join(config.data_path, 'processed_data/labels.npy')\n",
        "\n",
        "if os.path.exists(frames_path) and os.path.exists(labels_path):\n",
        "    histogram_frames = np.load(frames_path)\n",
        "    frame_labels = np.load(labels_path)\n",
        "    logger.info(f\"Loaded preprocessed data: {len(histogram_frames)} frames\")\n",
        "else:\n",
        "    aedat_file = os.path.join(config.data_path, 'DVS/DvsGesture/user10_fluorescent_led.aedat')\n",
        "    csv_file = os.path.join(config.data_path, 'DVS/DvsGesture/user10_fluorescent_led_labels.csv')\n",
        "    if os.path.exists(aedat_file) and os.path.exists(csv_file):\n",
        "        try:\n",
        "            events, labels = dvs_processor.load_dvs_gesture(aedat_file, csv_file)\n",
        "            if events is not None and len(labels) > 0:\n",
        "                event_stats = analytics.analyze_event_statistics(events)\n",
        "                logger.info(f\"Event statistics: {event_stats}\")\n",
        "                clean_events, noise_stats = dvs_processor.detect_noise_advanced(events)\n",
        "                logger.info(f\"Noise detection stats: {noise_stats}\")\n",
        "                augmented_events = augmenter.spatial_jitter(clean_events, max_shift=3)\n",
        "                augmented_events = augmenter.temporal_stretch(augmented_events, stretch_factor=1.1)\n",
        "                processed_events = dvs_processor.preprocess_dvs_gesture(augmented_events, labels)\n",
        "                dvs_processor.visualize_dvs_data(processed_events, labels, save_path='dvs_visualization.png')\n",
        "                histogram_frames = frame_converter.events_to_frames(\n",
        "                    processed_events,\n",
        "                    (config.dvs_max_y, config.dvs_max_x, 2),\n",
        "                    method='histogram'\n",
        "                )\n",
        "                logger.info(f\"Generated {len(histogram_frames)} histogram frames\")\n",
        "                # Improved label mapping\n",
        "                frame_labels = []\n",
        "                time_bins = np.arange(0, 1 + config.time_bin, config.time_bin)\n",
        "                t_min, t_max = events[:, 0].min(), events[:, 0].max()\n",
        "                for i in range(len(time_bins) - 1):\n",
        "                    t_start = time_bins[i]\n",
        "                    t_end = time_bins[i + 1]\n",
        "                    event_mask = (processed_events[:, 0] >= t_start) & (processed_events[:, 0] < t_end)\n",
        "                    if np.any(event_mask):\n",
        "                        label_found = False\n",
        "                        for _, row in labels.iterrows():\n",
        "                            start_time = (row['startTime_usec'] - t_min) / (t_max - t_min)\n",
        "                            end_time = (row['endTime_usec'] - t_min) / (t_max - t_min)\n",
        "                            if start_time <= t_end and end_time >= t_start:\n",
        "                                frame_labels.append(row['class'])\n",
        "                                label_found = True\n",
        "                                break\n",
        "                        if not label_found:\n",
        "                            frame_labels.append(-1)\n",
        "                    else:\n",
        "                        frame_labels.append(-1)\n",
        "                frame_labels = np.array(frame_labels[:len(histogram_frames)])\n",
        "                valid_mask = frame_labels != -1\n",
        "                histogram_frames = histogram_frames[valid_mask]\n",
        "                frame_labels = frame_labels[valid_mask]\n",
        "                if len(histogram_frames) == 0:\n",
        "                    raise Exception(\"No valid frames after label mapping\")\n",
        "            else:\n",
        "                raise Exception(\"Failed to load DVS data\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to process DVS data: {e}. Falling back to demo pipeline.\")\n",
        "            histogram_frames = None\n",
        "            frame_labels = None\n",
        "    else:\n",
        "        logger.warning(f\"DVS files not found at {aedat_file} or {csv_file}. Falling back to demo pipeline.\")\n",
        "        histogram_frames = None\n",
        "        frame_labels = None"
      ],
      "metadata": {
        "id": "U9eyk8fWorpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Train SNN Model\n",
        "if 'histogram_frames' in locals() and len(histogram_frames) > 0:\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        histogram_frames, frame_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "    train_dataset = NeuromorphicDataset(X_train, y_train)\n",
        "    val_dataset = NeuromorphicDataset(X_val, y_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "    input_shape = (2, config.dvs_max_y, config.dvs_max_x)\n",
        "    model = EnhancedSpikingNet(input_shape, config.dvs_num_classes)\n",
        "    trainer = Trainer(model, config)\n",
        "    history = trainer.train(train_loader, val_loader)\n",
        "    trainer.plot_training_history(save_path='training_history.png')\n",
        "    model_path = os.path.join(config.data_path, 'best_model.pth')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    gesture_mapping_file = os.path.join(config.data_path, 'DVS/DvsGesture/gesture_mapping.csv')\n",
        "    if os.path.exists(gesture_mapping_file):\n",
        "        gesture_mapping = pd.read_csv(gesture_mapping_file)\n",
        "        class_names = gesture_mapping['labels'].tolist()  # Adjust column name if needed\n",
        "    else:\n",
        "        class_names = [f\"Class_{i}\" for i in range(config.dvs_num_classes)]\n",
        "    metadata = {\n",
        "        'input_shape': input_shape,\n",
        "        'num_classes': config.dvs_num_classes,\n",
        "        'class_names': class_names\n",
        "    }\n",
        "    with open(os.path.join(config.data_path, 'model_metadata.pkl'), 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "    logger.info(f\"Model saved at {model_path}\")\n",
        "else:\n",
        "    logger.warning(\"No frames available for training\")"
      ],
      "metadata": {
        "id": "dcF-ZJJIpDan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Evaluate Model\n",
        "if 'val_dataset' in locals():\n",
        "    test_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "    class_names = metadata['class_names']\n",
        "    evaluation = analytics.evaluate_model(model, test_loader, class_names=class_names)\n",
        "    analytics.plot_confusion_matrix(evaluation['confusion_matrix'], class_names,\n",
        "                           save_path='confusion_matrix.png')\n",
        "    logger.info(f\"Test Accuracy: {evaluation['accuracy']:.4f}\")\n",
        "    logger.info(f\"Classification Report: {evaluation['classification_report']}\")\n",
        "else:\n",
        "    logger.warning(\"No test data available for evaluation\")"
      ],
      "metadata": {
        "id": "COgOnuvwpJca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Demo Pipeline (Synthetic Data)\n",
        "if 'histogram_frames' not in locals() or len(histogram_frames) == 0:\n",
        "    logger.info(\"Running demo pipeline with synthetic data\")\n",
        "    demo_results = create_demo_pipeline()\n",
        "    model_path = os.path.join(config.data_path, 'demo_model.pth')\n",
        "    torch.save(demo_results['model'].state_dict(), model_path)\n",
        "    metadata = {\n",
        "        'input_shape': (2, config.dvs_max_y, config.dvs_max_x),\n",
        "        'num_classes': 5,\n",
        "        'class_names': [f\"Synthetic_{i}\" for i in range(5)]\n",
        "    }\n",
        "    with open(os.path.join(config.data_path, 'demo_model_metadata.pkl'), 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "    logger.info(f\"Demo model saved at {model_path}\")"
      ],
      "metadata": {
        "id": "-ikXRNkupMnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Batch Processing (Optional)\n",
        "batch_processor = BatchProcessor(config)\n",
        "output_dir = os.path.join(config.data_path, 'processed_data')\n",
        "batch_results = batch_processor.process_dvs_dataset_batch(\n",
        "    os.path.join(config.data_path, 'DVS/DvsGesture'),\n",
        "    output_dir\n",
        ")\n",
        "logger.info(f\"Batch processing results: {batch_results}\")"
      ],
      "metadata": {
        "id": "D-FTXD1HpOXg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}