{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGpTGpIU/JXdy8NkUq4tGa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akobabs/Neuromorphic-Vision/blob/main/Copy_of_Neuromorphic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "%matplotlib inline\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Install required Python packages\n",
        "try:\n",
        "    import aedat\n",
        "except ImportError:\n",
        "    !pip install numpy pandas matplotlib torch norse scikit-learn seaborn tqdm aedat\n",
        "    import aedat\n",
        "\n",
        "# Mount Google Drive (for Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    logger.info(\"Not running in Colab, skipping drive mount\")\n",
        "\n",
        "# Verify data path\n",
        "data_path = '/content/drive/MyDrive/Neuromorphic Vision'\n",
        "if not os.path.exists(data_path):\n",
        "    logger.error(f\"Data path {data_path} does not exist\")\n",
        "else:\n",
        "    logger.info(f\"Data path {data_path} verified\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCGOQ8radxVx",
        "outputId": "e51d5eb1-739c-489d-9753-fd8948e2b6b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: norse in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting aedat\n",
            "  Using cached aedat-2.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from norse) (0.21.0+cu124)\n",
            "Requirement already satisfied: nir in /usr/local/lib/python3.11/dist-packages (from norse) (1.0.5)\n",
            "Requirement already satisfied: nirtorch in /usr/local/lib/python3.11/dist-packages (from norse) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from nir->norse) (3.13.0)\n",
            "Using cached aedat-2.2.0-cp311-cp311-manylinux_2_28_x86_64.whl (498 kB)\n",
            "Installing collected packages: aedat\n",
            "Successfully installed aedat-2.2.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Configuration\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration class for the neuromorphic vision pipeline\"\"\"\n",
        "    # Paths\n",
        "    data_path: str = '/content/drive/MyDrive/Neuromorphic Vision'\n",
        "\n",
        "    # DVS Gesture parameters\n",
        "    dvs_max_x: int = 128\n",
        "    dvs_max_y: int = 128\n",
        "    dvs_num_classes: int = 11\n",
        "\n",
        "    # N-Caltech101 parameters\n",
        "    caltech_max_x: int = 304\n",
        "    caltech_max_y: int = 240\n",
        "    caltech_num_classes: int = 101\n",
        "\n",
        "    # Preprocessing parameters\n",
        "    max_jitter: int = 100\n",
        "    time_bin: float = 0.01\n",
        "    temporal_window: float = 50000  # microseconds\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size: int = 4\n",
        "    learning_rate: float = 1e-3\n",
        "    num_epochs: int = 50\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Noise detection\n",
        "    dbscan_eps: float = 5.0\n",
        "    dbscan_min_samples: int = 10\n",
        "\n",
        "config = Config()\n",
        "logger.info(f\"Configuration initialized with device: {config.device}\")"
      ],
      "metadata": {
        "id": "adhIQb12dylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Imports and Class Definitions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import norse.torch as norse\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Updated DVSGestureProcessor\n",
        "from typing import Tuple, Optional, List, Dict, Any\n",
        "\n",
        "class DVSGestureProcessor:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def load_dvs_gesture(self, aedat_file: str, csv_file: str) -> Tuple[Optional[np.ndarray], Optional[pd.DataFrame]]:\n",
        "        \"\"\"Load DVS gesture data using aedat library\"\"\"\n",
        "        try:\n",
        "            import aedat\n",
        "            decoder = aedat.Decoder(aedat_file)\n",
        "            events = []\n",
        "            for packet in decoder:\n",
        "                if 'events' in packet:\n",
        "                    for event in packet['events']:\n",
        "                        events.append([\n",
        "                            event['timestamp'],\n",
        "                            event['x'],\n",
        "                            event['y'],\n",
        "                            event['polarity']\n",
        "                        ])\n",
        "            event_array = np.array(events, dtype=np.float64)\n",
        "\n",
        "            # Load labels\n",
        "            labels = pd.read_csv(csv_file, names=['class', 'startTime_usec', 'endTime_usec'])\n",
        "\n",
        "            logger.info(f\"Loaded {len(event_array)} events and {len(labels)} labels from {aedat_file}\")\n",
        "            return event_array, labels\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading DVS gesture data from {aedat_file}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def detect_noise_advanced(self, events: np.ndarray) -> Tuple[np.ndarray, Dict[str, float]]:\n",
        "        \"\"\"Advanced noise detection with multiple criteria\"\"\"\n",
        "        noise_mask = np.zeros(len(events), dtype=bool)\n",
        "        stats = {}\n",
        "\n",
        "        # Spatial clustering for noise detection\n",
        "        coords = events[:, 1:3]\n",
        "        if len(coords) > 0:\n",
        "            clustering = DBSCAN(eps=self.config.dbscan_eps,\n",
        "                              min_samples=self.config.dbscan_min_samples).fit(coords)\n",
        "            spatial_noise = clustering.labels_ == -1\n",
        "            noise_mask |= spatial_noise\n",
        "            stats['spatial_noise_rate'] = np.mean(spatial_noise)\n",
        "\n",
        "        # Temporal noise detection (isolated events)\n",
        "        if len(events) > 1:\n",
        "            time_diffs = np.diff(events[:, 0])\n",
        "            temporal_threshold = np.percentile(time_diffs, 95)\n",
        "            isolated_events = np.concatenate([[False], time_diffs > temporal_threshold])\n",
        "            noise_mask |= isolated_events\n",
        "            stats['temporal_noise_rate'] = np.mean(isolated_events)\n",
        "\n",
        "        # Hot pixel detection\n",
        "        pixel_counts = {}\n",
        "        for x, y in events[:, 1:3]:\n",
        "            pixel_counts[(x, y)] = pixel_counts.get((x, y), 0) + 1\n",
        "\n",
        "        if pixel_counts:\n",
        "            count_threshold = np.percentile(list(pixel_counts.values()), 99)\n",
        "            hot_pixel_mask = np.array([pixel_counts[(x, y)] > count_threshold\n",
        "                                     for x, y in events[:, 1:3]])\n",
        "            noise_mask |= hot_pixel_mask\n",
        "            stats['hot_pixel_rate'] = np.mean(hot_pixel_mask)\n",
        "\n",
        "        stats['total_noise_rate'] = np.mean(noise_mask)\n",
        "        logger.info(f\"Noise detection stats: {stats}\")\n",
        "\n",
        "        return events[~noise_mask], stats\n",
        "\n",
        "    def preprocess_dvs_gesture(self, events: np.ndarray, labels: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Enhanced preprocessing with better temporal handling\"\"\"\n",
        "        if events is None or len(events) == 0:\n",
        "            return np.array([])\n",
        "\n",
        "        # Filter events within label windows\n",
        "        valid_mask = np.zeros(len(events), dtype=bool)\n",
        "        for _, row in labels.iterrows():\n",
        "            start_time, end_time = row['startTime_usec'], row['endTime_usec']\n",
        "            window_mask = (events[:, 0] >= start_time) & (events[:, 0] <= end_time)\n",
        "            valid_mask |= window_mask\n",
        "\n",
        "        events = events[valid_mask]\n",
        "\n",
        "        # Spatial filtering\n",
        "        spatial_mask = ((events[:, 1] >= 0) & (events[:, 1] < self.config.dvs_max_x) &\n",
        "                       (events[:, 2] >= 0) & (events[:, 2] < self.config.dvs_max_y))\n",
        "        events = events[spatial_mask]\n",
        "\n",
        "        if len(events) == 0:\n",
        "            return events\n",
        "\n",
        "        # Temporal normalization with better handling\n",
        "        t_min, t_max = events[:, 0].min(), events[:, 0].max()\n",
        "        if t_max > t_min:\n",
        "            events[:, 0] = (events[:, 0] - t_min) / (t_max - t_min)\n",
        "\n",
        "            # Add temporal jitter for augmentation\n",
        "            if self.config.max_jitter > 0:\n",
        "                jitter = np.random.uniform(-self.config.max_jitter / (t_max - t_min),\n",
        "                                         self.config.max_jitter / (t_max - t_min),\n",
        "                                         len(events))\n",
        "                events[:, 0] = np.clip(events[:, 0] + jitter, 0, 1)\n",
        "\n",
        "        return events\n",
        "\n",
        "    def visualize_dvs_data(self, events: np.ndarray, labels: pd.DataFrame, save_path: Optional[str] = None):\n",
        "        \"\"\"Enhanced visualization with subplots\"\"\"\n",
        "        if events is None or len(events) == 0:\n",
        "            logger.warning(\"No events to visualize\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # Temporal distribution\n",
        "        axes[0, 0].hist(events[:, 0], bins=100, color='blue', alpha=0.7, edgecolor='black')\n",
        "        axes[0, 0].set_xlabel('Normalized Timestamp')\n",
        "        axes[0, 0].set_ylabel('Event Count')\n",
        "        axes[0, 0].set_title('DVS Gesture Temporal Distribution')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Spatial distribution\n",
        "        scatter = axes[0, 1].scatter(events[:, 1], events[:, 2], s=0.5,\n",
        "                                   c=events[:, 3], cmap='Blues', alpha=0.6)\n",
        "        axes[0, 1].set_xlabel('X (pixels)')\n",
        "        axes[0, 1].set_ylabel('Y (pixels)')\n",
        "        axes[0, 1].set_title('DVS Gesture Spatial Distribution')\n",
        "        plt.colorbar(scatter, ax=axes[0, 1], label='Polarity (0: OFF, 1: ON)')\n",
        "\n",
        "        # Class distribution\n",
        "        class_counts = labels['class'].value_counts().sort_index()\n",
        "        axes[1, 0].bar(class_counts.index, class_counts.values,\n",
        "                      color='skyblue', edgecolor='black')\n",
        "        axes[1, 0].set_xlabel('Gesture Class')\n",
        "        axes[1, 0].set_ylabel('Count')\n",
        "        axes[1, 0].set_title('DVS Gesture Class Distribution')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Event rate over time\n",
        "        time_bins = np.linspace(0, 1, 50)\n",
        "        event_rates = []\n",
        "        for i in range(len(time_bins) - 1):\n",
        "            mask = (events[:, 0] >= time_bins[i]) & (events[:, 0] < time_bins[i + 1])\n",
        "            event_rates.append(np.sum(mask))\n",
        "\n",
        "        axes[1, 1].plot(time_bins[:-1], event_rates, color='blue', linewidth=2)\n",
        "        axes[1, 1].set_xlabel('Normalized Time')\n",
        "        axes[1, 1].set_ylabel('Event Rate')\n",
        "        axes[1, 1].set_title('Event Rate Over Time')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "logger.info(\"All classes defined\")"
      ],
      "metadata": {
        "id": "eAmAWNG_d3Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - DataProcessor\n",
        "class DataProcessor:\n",
        "    \"\"\"Enhanced data processing utilities\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def setup_environment():\n",
        "        \"\"\"Setup environment and install required packages\"\"\"\n",
        "        try:\n",
        "            import loris\n",
        "        except ImportError:\n",
        "            os.system('pip install loris numpy pandas matplotlib torch norse scikit-learn seaborn tqdm')\n",
        "            import loris\n",
        "\n",
        "        # Mount Google Drive if in Colab\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "        except ImportError:\n",
        "            logger.info(\"Not running in Colab, skipping drive mount\")\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_file_paths(file_paths: List[str]) -> List[str]:\n",
        "        \"\"\"Validate that all file paths exist\"\"\"\n",
        "        valid_paths = []\n",
        "        for path in file_paths:\n",
        "            if os.path.exists(path):\n",
        "                valid_paths.append(path)\n",
        "            else:\n",
        "                logger.warning(f\"File not found: {path}\")\n",
        "        return valid_paths"
      ],
      "metadata": {
        "id": "pYZiUkyOiJ_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - NCaltech101Processor\n",
        "class NCaltech101Processor:\n",
        "    \"\"\"Enhanced N-Caltech101 dataset processor\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def load_ncaltech101(self, bin_file: str) -> np.ndarray:\n",
        "        \"\"\"Load N-Caltech101 data with better error handling\"\"\"\n",
        "        try:\n",
        "            with open(bin_file, 'rb') as f:\n",
        "                data = f.read()\n",
        "\n",
        "            # More efficient unpacking\n",
        "            events = []\n",
        "            for i in range(0, len(data), 5):\n",
        "                if i + 5 <= len(data):\n",
        "                    event = struct.unpack('<BBHB', data[i:i+5])  # Little endian\n",
        "                    x, y = event[0], event[1]\n",
        "                    polarity = (event[2] >> 7) & 0x01\n",
        "                    timestamp = ((event[2] & 0x7F) << 16) | event[3]\n",
        "                    events.append([timestamp, x, y, polarity])\n",
        "\n",
        "            events = np.array(events, dtype=np.float32)\n",
        "            logger.info(f\"Loaded {len(events)} events from {bin_file}\")\n",
        "            return events\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading N-Caltech101 data from {bin_file}: {e}\")\n",
        "            return np.array([])\n",
        "\n",
        "    def preprocess_ncaltech101(self, events: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Enhanced preprocessing for N-Caltech101\"\"\"\n",
        "        if len(events) == 0:\n",
        "            return events\n",
        "\n",
        "        # Spatial filtering\n",
        "        spatial_mask = ((events[:, 1] >= 0) & (events[:, 1] < self.config.caltech_max_x) &\n",
        "                       (events[:, 2] >= 0) & (events[:, 2] < self.config.caltech_max_y))\n",
        "        events = events[spatial_mask]\n",
        "\n",
        "        if len(events) == 0:\n",
        "            return events\n",
        "\n",
        "        # Temporal normalization\n",
        "        t_min, t_max = events[:, 0].min(), events[:, 0].max()\n",
        "        if t_max > t_min:\n",
        "            events[:, 0] = (events[:, 0] - t_min) / (t_max - t_min)\n",
        "\n",
        "            # Add temporal jitter\n",
        "            if self.config.max_jitter > 0:\n",
        "                jitter = np.random.uniform(-self.config.max_jitter / (t_max - t_min),\n",
        "                                         self.config.max_jitter / (t_max - t_min),\n",
        "                                         len(events))\n",
        "                events[:, 0] = np.clip(events[:, 0] + jitter, 0, 1)\n",
        "\n",
        "        return events"
      ],
      "metadata": {
        "id": "H87AJ-TngSXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - EventFrameConverter\n",
        "class EventFrameConverter:\n",
        "    \"\"\"Enhanced event-to-frame conversion with multiple representations\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def events_to_frames(self, events: np.ndarray, shape: Tuple[int, int, int],\n",
        "                        method: str = 'histogram') -> np.ndarray:\n",
        "        \"\"\"Convert events to frames with multiple methods\"\"\"\n",
        "        if len(events) == 0:\n",
        "            return np.zeros((1, *shape))\n",
        "\n",
        "        time_bins = np.arange(0, 1 + self.config.time_bin, self.config.time_bin)\n",
        "        frames = []\n",
        "\n",
        "        for i in range(len(time_bins) - 1):\n",
        "            t_start, t_end = time_bins[i], time_bins[i + 1]\n",
        "            mask = (events[:, 0] >= t_start) & (events[:, 0] < t_end)\n",
        "            frame_events = events[mask]\n",
        "\n",
        "            if method == 'histogram':\n",
        "                frame = self._histogram_representation(frame_events, shape)\n",
        "            elif method == 'time_surface':\n",
        "                frame = self._time_surface_representation(frame_events, shape, t_end)\n",
        "            else:\n",
        "                frame = self._histogram_representation(frame_events, shape)\n",
        "\n",
        "            frames.append(frame)\n",
        "\n",
        "        return np.array(frames)\n",
        "\n",
        "    def _histogram_representation(self, events: np.ndarray, shape: Tuple[int, int, int]) -> np.ndarray:\n",
        "        \"\"\"Standard histogram representation\"\"\"\n",
        "        frame = np.zeros(shape)\n",
        "        if len(events) > 0:\n",
        "            for _, x, y, p in events:\n",
        "                if 0 <= int(x) < shape[1] and 0 <= int(y) < shape[0]:\n",
        "                    frame[int(y), int(x), int(p)] += 1\n",
        "        return frame\n",
        "\n",
        "    def _time_surface_representation(self, events: np.ndarray, shape: Tuple[int, int, int],\n",
        "                                   current_time: float) -> np.ndarray:\n",
        "        \"\"\"Time surface representation\"\"\"\n",
        "        frame = np.zeros(shape)\n",
        "        if len(events) > 0:\n",
        "            for t, x, y, p in events:\n",
        "                if 0 <= int(x) < shape[1] and 0 <= int(y) < shape[0]:\n",
        "                    # Exponential decay based on time difference\n",
        "                    time_diff = current_time - t\n",
        "                    decay = np.exp(-time_diff / 0.1)  # Decay constant\n",
        "                    frame[int(y), int(x), int(p)] = max(frame[int(y), int(x), int(p)], decay)\n",
        "        return frame"
      ],
      "metadata": {
        "id": "1t6WZYhDfxXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - NeuromorphicDataset\n",
        "class NeuromorphicDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for neuromorphic data\"\"\"\n",
        "\n",
        "    def __init__(self, frames: np.ndarray, labels: np.ndarray, transform=None):\n",
        "        self.frames = torch.FloatTensor(frames)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame = self.frames[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            frame = self.transform(frame)\n",
        "\n",
        "        return frame, label\n"
      ],
      "metadata": {
        "id": "Rv6GtzA5gvaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - EnhancedSpikingNet\n",
        "class EnhancedSpikingNet(nn.Module):\n",
        "    \"\"\"Enhanced Spiking Neural Network with better architecture\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape: Tuple[int, int, int], num_classes: int, dropout_rate: float = 0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        c, h, w = input_shape\n",
        "\n",
        "        # Feature extraction layers\n",
        "        self.features = nn.Sequential(\n",
        "            # First conv block\n",
        "            norse.LIConv2d(c, 32, kernel_size=3, padding=1),\n",
        "            norse.LIFCell(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(dropout_rate),\n",
        "\n",
        "            # Second conv block\n",
        "            norse.LIConv2d(32, 64, kernel_size=3, padding=1),\n",
        "            norse.LIFCell(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(dropout_rate),\n",
        "\n",
        "            # Third conv block\n",
        "            norse.LIConv2d(64, 128, kernel_size=3, padding=1),\n",
        "            norse.LIFCell(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.AdaptiveAvgPool2d((4, 4)),\n",
        "            nn.Dropout2d(dropout_rate),\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            norse.LIFCell(128 * 4 * 4, 256),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            norse.LIFCell(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-H4s4_m6g_hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - Trainer\n",
        "class Trainer:\n",
        "    \"\"\"Enhanced training class with better monitoring\"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, config: Config):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = torch.device(config.device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def train_epoch(self, dataloader: DataLoader) -> Tuple[float, float]:\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "            # Update progress bar\n",
        "            accuracy = 100. * correct / total\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Acc': f'{accuracy:.2f}%'\n",
        "            })\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        accuracy = 100. * correct / total\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def validate(self, dataloader: DataLoader) -> Tuple[float, float]:\n",
        "        \"\"\"Validate the model\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in tqdm(dataloader, desc=\"Validation\"):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, target)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "                total += target.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        accuracy = 100. * correct / total\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def train(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[str, List[float]]:\n",
        "        \"\"\"Full training loop with monitoring\"\"\"\n",
        "        logger.info(f\"Starting training on {self.device}\")\n",
        "\n",
        "        best_val_acc = 0\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            logger.info(f\"Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_acc = self.train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_acc)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc = self.validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Early stopping\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= 10:\n",
        "                    logger.info(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "            logger.info(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "            logger.info(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "            logger.info(f\"Best Val Acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "        return {\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "            'train_accuracies': self.train_accuracies,\n",
        "            'val_accuracies': self.val_accuracies\n",
        "        }"
      ],
      "metadata": {
        "id": "nPZAHYBthD_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - AdvancedAnalytics\n",
        "class AdvancedAnalytics:\n",
        "    \"\"\"Advanced analytics and evaluation tools\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def evaluate_model(self, model: nn.Module, test_loader: DataLoader,\n",
        "                      class_names: List[str] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        model.eval()\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                probs = torch.softmax(output, dim=1)\n",
        "                pred = output.argmax(dim=1)\n",
        "\n",
        "                all_preds.extend(pred.cpu().numpy())\n",
        "                all_targets.extend(target.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = np.mean(np.array(all_preds) == np.array(all_targets))\n",
        "\n",
        "        # Classification report\n",
        "        if class_names is None:\n",
        "            class_names = [f\"Class_{i}\" for i in range(len(np.unique(all_targets)))]\n",
        "\n",
        "        report = classification_report(all_targets, all_preds,\n",
        "                                     target_names=class_names,\n",
        "                                     output_dict=True)\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'classification_report': report,\n",
        "            'confusion_matrix': cm,\n",
        "            'predictions': all_preds,\n",
        "            'targets': all_targets,\n",
        "            'probabilities': all_probs\n",
        "        }\n",
        "\n",
        "    def plot_confusion_matrix(self, cm: np.ndarray, class_names: List[str],\n",
        "                            save_path: Optional[str] = None):\n",
        "        \"\"\"Plot confusion matrix with better visualization\"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def analyze_event_statistics(self, events: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"Comprehensive event statistics analysis\"\"\"\n",
        "        if len(events) == 0:\n",
        "            return {}\n",
        "\n",
        "        stats = {\n",
        "            'total_events': len(events),\n",
        "            'temporal_span': events[:, 0].max() - events[:, 0].min(),\n",
        "            'spatial_span_x': events[:, 1].max() - events[:, 1].min(),\n",
        "            'spatial_span_y': events[:, 2].max() - events[:, 2].min(),\n",
        "            'polarity_ratio': np.mean(events[:, 3]),\n",
        "            'event_rate': len(events) / (events[:, 0].max() - events[:, 0].min() + 1e-6),\n",
        "            'spatial_density': len(events) / ((events[:, 1].max() - events[:, 1].min() + 1) *\n",
        "                                            (events[:, 2].max() - events[:, 2].min() + 1))\n",
        "        }\n",
        "\n",
        "        # Temporal statistics\n",
        "        if len(events) > 1:\n",
        "            inter_event_times = np.diff(events[:, 0])\n",
        "            stats.update({\n",
        "                'mean_inter_event_time': np.mean(inter_event_times),\n",
        "                'std_inter_event_time': np.std(inter_event_times),\n",
        "                'median_inter_event_time': np.median(inter_event_times)\n",
        "            })\n",
        "\n",
        "        return stats"
      ],
      "metadata": {
        "id": "JoEQlCrrhZD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - DataAugmentation\n",
        "class DataAugmentation:\n",
        "    \"\"\"Advanced data augmentation techniques for neuromorphic data\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def spatial_jitter(events: np.ndarray, max_shift: int = 5) -> np.ndarray:\n",
        "        \"\"\"Apply spatial jitter to events\"\"\"\n",
        "        augmented_events = events.copy()\n",
        "        shift_x = np.random.randint(-max_shift, max_shift + 1)\n",
        "        shift_y = np.random.randint(-max_shift, max_shift + 1)\n",
        "\n",
        "        augmented_events[:, 1] += shift_x\n",
        "        augmented_events[:, 2] += shift_y\n",
        "\n",
        "        return augmented_events\n",
        "\n",
        "    @staticmethod\n",
        "    def temporal_stretch(events: np.ndarray, stretch_factor: float = 1.2) -> np.ndarray:\n",
        "        \"\"\"Apply temporal stretching/compression\"\"\"\n",
        "        augmented_events = events.copy()\n",
        "        augmented_events[:, 0] *= stretch_factor\n",
        "        return augmented_events\n",
        "\n",
        "    @staticmethod\n",
        "    def polarity_flip(events: np.ndarray, flip_prob: float = 0.1) -> np.ndarray:\n",
        "        \"\"\"Randomly flip polarity of some events\"\"\"\n",
        "        augmented_events = events.copy()\n",
        "        flip_mask = np.random.random(len(events)) < flip_prob\n",
        "        augmented_events[flip_mask, 3] = 1 - augmented_events[flip_mask, 3]\n",
        "        return augmented_events\n",
        "\n",
        "    @staticmethod\n",
        "    def event_dropout(events: np.ndarray, dropout_rate: float = 0.1) -> np.ndarray:\n",
        "        \"\"\"Randomly drop events\"\"\"\n",
        "        keep_mask = np.random.random(len(events)) > dropout_rate\n",
        "        return events[keep_mask]"
      ],
      "metadata": {
        "id": "4gGoSIqYhmR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - BatchProcessor\n",
        "class BatchProcessor:\n",
        "    \"\"\"Batch processing utilities for large datasets\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def process_dvs_dataset_batch(self, data_dir: str, output_dir: str,\n",
        "                                num_workers: int = 4) -> Dict[str, Any]:\n",
        "        \"\"\"Process entire DVS dataset in batches\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Find all AEDAT files\n",
        "        aedat_files = []\n",
        "        for root, dirs, files in os.walk(data_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.aedat'):\n",
        "                    aedat_path = os.path.join(root, file)\n",
        "                    csv_path = aedat_path.replace('.aedat', '_labels.csv')\n",
        "                    if os.path.exists(csv_path):\n",
        "                        aedat_files.append((aedat_path, csv_path))\n",
        "\n",
        "        logger.info(f\"Found {len(aedat_files)} AEDAT files with labels\")\n",
        "\n",
        "        # Process files in parallel\n",
        "        processor = DVSGestureProcessor(self.config)\n",
        "        frame_converter = EventFrameConverter(self.config)\n",
        "\n",
        "        all_frames = []\n",
        "        all_labels = []\n",
        "        processing_stats = []\n",
        "\n",
        "        def process_single_file(file_pair):\n",
        "            aedat_file, csv_file = file_pair\n",
        "            try:\n",
        "                # Load data\n",
        "                events, labels = processor.load_dvs_gesture(aedat_file, csv_file)\n",
        "                if events is None:\n",
        "                    return None\n",
        "\n",
        "                # Process events\n",
        "                clean_events, noise_stats = processor.detect_noise_advanced(events)\n",
        "                processed_events = processor.preprocess_dvs_gesture(clean_events, labels)\n",
        "\n",
        "                # Convert to frames\n",
        "                frames = frame_converter.events_to_frames(\n",
        "                    processed_events,\n",
        "                    (self.config.dvs_max_y, self.config.dvs_max_x, 2)\n",
        "                )\n",
        "\n",
        "                # Create labels for frames\n",
        "                frame_labels = []\n",
        "                for _, row in labels.iterrows():\n",
        "                    frame_labels.extend([row['class']] * (len(frames) // len(labels)))\n",
        "\n",
        "                return {\n",
        "                    'frames': frames,\n",
        "                    'labels': frame_labels[:len(frames)],\n",
        "                    'stats': noise_stats,\n",
        "                    'file': aedat_file\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing {aedat_file}: {e}\")\n",
        "                return None\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel processing\n",
        "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "            results = list(tqdm(\n",
        "                executor.map(process_single_file, aedat_files),\n",
        "                total=len(aedat_files),\n",
        "                desc=\"Processing files\"\n",
        "            ))\n",
        "\n",
        "        # Aggregate results\n",
        "        valid_results = [r for r in results if r is not None]\n",
        "\n",
        "        for result in valid_results:\n",
        "            all_frames.append(result['frames'])\n",
        "            all_labels.extend(result['labels'])\n",
        "            processing_stats.append(result['stats'])\n",
        "\n",
        "        if all_frames:\n",
        "            # Concatenate all frames\n",
        "            all_frames = np.concatenate(all_frames, axis=0)\n",
        "            all_labels = np.array(all_labels)\n",
        "\n",
        "            # Save processed data\n",
        "            np.save(os.path.join(output_dir, 'frames.npy'), all_frames)\n",
        "            np.save(os.path.join(output_dir, 'labels.npy'), all_labels)\n",
        "\n",
        "            # Save processing statistics\n",
        "            with open(os.path.join(output_dir, 'processing_stats.pkl'), 'wb') as f:\n",
        "                pickle.dump(processing_stats, f)\n",
        "\n",
        "            logger.info(f\"Processed {len(all_frames)} frames from {len(valid_results)} files\")\n",
        "\n",
        "            return {\n",
        "                'frames': all_frames,\n",
        "                'labels': all_labels,\n",
        "                'stats': processing_stats,\n",
        "                'num_files': len(valid_results)\n",
        "            }\n",
        "\n",
        "        return {}"
      ],
      "metadata": {
        "id": "r2i93kVzhvoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - ModelOptimizer\n",
        "class ModelOptimizer:\n",
        "    \"\"\"Model optimization and hyperparameter tuning\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def architecture_search(self, train_loader: DataLoader, val_loader: DataLoader,\n",
        "                          input_shape: Tuple[int, int, int], num_classes: int) -> Dict[str, Any]:\n",
        "        \"\"\"Simple architecture search\"\"\"\n",
        "        architectures = [\n",
        "            {'channels': [16, 32, 64], 'dropout': 0.1},\n",
        "            {'channels': [32, 64, 128], 'dropout': 0.2},\n",
        "            {'channels': [64, 128, 256], 'dropout': 0.3},\n",
        "        ]\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i, arch_config in enumerate(architectures):\n",
        "            logger.info(f\"Testing architecture {i+1}/{len(architectures)}: {arch_config}\")\n",
        "\n",
        "            # Create model with specific architecture\n",
        "            model = self._create_custom_model(input_shape, num_classes, arch_config)\n",
        "            trainer = Trainer(model, self.config)\n",
        "\n",
        "            # Quick training (fewer epochs)\n",
        "            temp_config = Config()\n",
        "            temp_config.num_epochs = 10\n",
        "            trainer.config = temp_config\n",
        "\n",
        "            history = trainer.train(train_loader, val_loader)\n",
        "            best_val_acc = max(history['val_accuracies'])\n",
        "\n",
        "            results.append({\n",
        "                'architecture': arch_config,\n",
        "                'best_val_accuracy': best_val_acc,\n",
        "                'history': history\n",
        "            })\n",
        "\n",
        "        # Sort by best validation accuracy\n",
        "        results.sort(key=lambda x: x['best_val_accuracy'], reverse=True)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _create_custom_model(self, input_shape: Tuple[int, int, int],\n",
        "                           num_classes: int, arch_config: Dict[str, Any]) -> nn.Module:\n",
        "        \"\"\"Create model with custom architecture\"\"\"\n",
        "        class CustomSpikingNet(nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                c, h, w = input_shape\n",
        "                channels = arch_config['channels']\n",
        "                dropout = arch_config['dropout']\n",
        "\n",
        "                layers = []\n",
        "                in_channels = c\n",
        "\n",
        "                for out_channels in channels:\n",
        "                    layers.extend([\n",
        "                        norse.LIConv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                        norse.LIFCell(),\n",
        "                        nn.BatchNorm2d(out_channels),\n",
        "                        nn.MaxPool2d(2),\n",
        "                        nn.Dropout2d(dropout),\n",
        "                    ])\n",
        "                    in_channels = out_channels\n",
        "\n",
        "                self.features = nn.Sequential(*layers)\n",
        "\n",
        "                # Calculate flattened size\n",
        "                with torch.no_grad():\n",
        "                    dummy_input = torch.zeros(1, c, h, w)\n",
        "                    dummy_output = self.features(dummy_input)\n",
        "                    flattened_size = dummy_output.numel()\n",
        "\n",
        "                self.classifier = nn.Sequential(\n",
        "                    nn.Flatten(),\n",
        "                    norse.LIFCell(flattened_size, 256),\n",
        "                    nn.Dropout(dropout),\n",
        "                    norse.LIFCell(256, num_classes)\n",
        "                )\n",
        "\n",
        "            def forward(self, x):\n",
        "                x = self.features(x)\n",
        "                x = self.classifier(x)\n",
        "                return x\n",
        "\n",
        "        return CustomSpikingNet()"
      ],
      "metadata": {
        "id": "fGCm818Ph2QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Process DVS Gesture Data or Load Preprocessed\n",
        "dvs_processor = DVSGestureProcessor(config)\n",
        "frame_converter = EventFrameConverter(config)\n",
        "analytics = AdvancedAnalytics(config)\n",
        "augmenter = DataAugmentation()\n",
        "\n",
        "# Check for preprocessed data\n",
        "frames_path = os.path.join(config.data_path, 'processed_data/frames.npy')\n",
        "labels_path = os.path.join(config.data_path, 'processed_data/labels.npy')\n",
        "\n",
        "if os.path.exists(frames_path) and os.path.exists(labels_path):\n",
        "    histogram_frames = np.load(frames_path)\n",
        "    frame_labels = np.load(labels_path)\n",
        "    logger.info(f\"Loaded preprocessed data: {len(histogram_frames)} frames\")\n",
        "else:\n",
        "    aedat_file = os.path.join(config.data_path, 'DVS/DvsGesture/user10_fluorescent_led.aedat')\n",
        "    csv_file = os.path.join(config.data_path, 'DVS/DvsGesture/user10_fluorescent_led_labels.csv')\n",
        "    if os.path.exists(aedat_file) and os.path.exists(csv_file):\n",
        "        try:\n",
        "            events, labels = dvs_processor.load_dvs_gesture(aedat_file, csv_file)\n",
        "            if events is not None:\n",
        "                event_stats = analytics.analyze_event_statistics(events)\n",
        "                logger.info(f\"Event statistics: {event_stats}\")\n",
        "                clean_events = events\n",
        "                _, noise_stats = dvs_processor.detect_noise_advanced(clean_events)\n",
        "                logger.info(f\"Noise detection stats: {noise_stats}\")\n",
        "                augmented_events = augmenter.spatial_jitter(clean_events, max_shift=1)\n",
        "                augmented_events = augmenter.temporal_stretch(augmented_events, stretch_factor=1.1)\n",
        "                processed_events = dvs_processor.preprocess_dvs_gesture(augmented_events, labels)\n",
        "                dvs_processor.visualize_dvs_data(processed_events, labels, save_path='dvs_visualization.png')\n",
        "                histogram_frames = frame_converter.events_to_frames(\n",
        "                    processed_events,\n",
        "                    (config.dvs_max_y, config.dvs_max_x, 2),\n",
        "                    method='histogram'\n",
        "                )\n",
        "                logger.info(f\"Generated {len(histogram_frames)} histogram frames\")\n",
        "                # Map frame labels using time windows\n",
        "                frame_labels = []\n",
        "                time_bins = np.arange(0, 1 + config.time_bin, config.time_bin)\n",
        "                for i in range(len(time_bins) - 1):\n",
        "                    t_start = time_bins[i]\n",
        "                    t_end = time_bins[i + 1]\n",
        "                    event_mask = (processed_events[:, 0] >= t_start) & (processed_events[:, 0] < t_end)\n",
        "                    if np.any(event_mask):\n",
        "                        # Find corresponding label\n",
        "                        for _, row in labels.iterrows():\n",
        "                            start_time = (row['startTime_usec'] - events[:, 0].min()) / (events[:, 0].max() - events[:, 0].min())\n",
        "                            end_time = (row['endTime_usec'] - events[:, 0].min()) / (events[:, 0].max() - events[:, 0].min())\n",
        "                            if start_time <= t_end and end_time >= t_start:\n",
        "                                frame_labels.append(row['class'])\n",
        "                                break\n",
        "                        else:\n",
        "                            frame_labels.append(0)  # Default class if no overlap\n",
        "                    else:\n",
        "                        frame_labels.append(0)\n",
        "                frame_labels = np.array(frame_labels[:len(histogram_frames)])\n",
        "            else:\n",
        "                raise Exception(\"Failed to load DVS data\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to process DVS data: {e}. Falling back to demo pipeline.\")\n",
        "            histogram_frames = None\n",
        "    else:\n",
        "        logger.warning(f\"DVS files not found at {aedat_file} or {csv_file}. Falling back to demo pipeline.\")\n",
        "        histogram_frames = None"
      ],
      "metadata": {
        "id": "pyWeHz2Tg6mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Train SNN Model\n",
        "if 'histogram_frames' in locals() and len(histogram_frames) > 0:\n",
        "    # Split data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        histogram_frames, frame_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    train_dataset = NeuromorphicDataset(X_train, y_train)\n",
        "    val_dataset = NeuromorphicDataset(X_val, y_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    input_shape = (2, config.dvs_max_y, config.dvs_max_x)\n",
        "    model = EnhancedSpikingNet(input_shape, config.dvs_num_classes)\n",
        "\n",
        "    # Train model\n",
        "    trainer = Trainer(model, config)\n",
        "    history = trainer.train(train_loader, val_loader)\n",
        "\n",
        "    # Plot training history\n",
        "    trainer.plot_training_history(save_path='training_history.png')\n",
        "\n",
        "    # Save model and metadata\n",
        "    model_path = os.path.join(config.data_path, 'best_model.pth')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # Load gesture mapping for class names\n",
        "    gesture_mapping_file = os.path.join(config.data_path, 'DVS/gesture_mapping.csv')\n",
        "    if os.path.exists(gesture_mapping_file):\n",
        "        gesture_mapping = pd.read_csv(gesture_mapping_file)\n",
        "        class_names = gesture_mapping['gesture'].tolist()  # Adjust column name if needed\n",
        "    else:\n",
        "        class_names = [f\"Class_{i}\" for i in range(config.dvs_num_classes)]\n",
        "\n",
        "    metadata = {\n",
        "        'input_shape': input_shape,\n",
        "        'num_classes': config.dvs_num_classes,\n",
        "        'class_names': class_names\n",
        "    }\n",
        "    with open(os.path.join(config.data_path, 'model_metadata.pkl'), 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    logger.info(f\"Model saved at {model_path}\")\n",
        "else:\n",
        "    logger.warning(\"No frames available for training\")"
      ],
      "metadata": {
        "id": "K3muz2ZTg-6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Evaluate Model\n",
        "if 'val_dataset' in locals():\n",
        "    test_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "    class_names = metadata['class_names']\n",
        "\n",
        "    evaluation = analytics.evaluate_model(model, test_loader, class_names=class_names)\n",
        "    analytics.plot_confusion_matrix(evaluation['confusion_matrix'], class_names,\n",
        "                                  save_path='confusion_matrix.png')\n",
        "\n",
        "    logger.info(f\"Test Accuracy: {evaluation['accuracy']:.4f}\")\n",
        "    logger.info(f\"Classification Report: {evaluation['classification_report']}\")\n",
        "else:\n",
        "    logger.warning(\"No test data available for evaluation\")"
      ],
      "metadata": {
        "id": "uicejHAPiajD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Demo Pipeline (Synthetic Data)\n",
        "if 'histogram_frames' not in locals() or len(histogram_frames) == 0:\n",
        "    logger.info(\"Running demo pipeline with synthetic data\")\n",
        "    demo_results = create_demo_pipeline()\n",
        "\n",
        "    # Save demo model and metadata\n",
        "    model_path = os.path.join(config.data_path, 'demo_model.pth')\n",
        "    torch.save(demo_results['model'].state_dict(), model_path)\n",
        "\n",
        "    metadata = {\n",
        "        'input_shape': (2, config.dvs_max_y, config.dvs_max_x),\n",
        "        'num_classes': 5,\n",
        "        'class_names': [f\"Synthetic_Class_{i}\" for i in range(5)]\n",
        "    }\n",
        "    with open(os.path.join(config.data_path, 'demo_model_metadata.pkl'), 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    logger.info(f\"Demo model saved at {model_path}\")"
      ],
      "metadata": {
        "id": "DPEu1Bqzibid"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}